{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 9 \n",
    "\n",
    "In this lab, we will see how we can develop and train a Convolutional Neural Network using the Python library Keras. We will use the movie reviews provided on moodle (training.txt) which we also used last week. \n",
    "\n",
    "When implementing a Convolutional Neural Network:\n",
    "1. We firstly need to split the data into training and testing and then we need to extract features (or vice versa). \n",
    "2. Then we need to define the architecture of the convolutional neural network.\n",
    "3. Then we need to train the model, and finally,\n",
    "4. Evaluate it (more on this at Unit 10)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Firstly, you will need to load the dataset, in this case, the dataset is provided on moodle and it contains movie reviews.  In this example, we will use pandas to load the data (there are other ways to load a .txt file as we saw last week). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the data is saved as a dataframe with two column names: label and review\n",
    "df = pd.read_csv('training.txt', names=['label', 'review'], sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can see how the first example looks like below\n",
    "print(df.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We store the reviews and labels in two arrays as follows:\n",
    "reviews = df['review'].values\n",
    "labels = df['label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then, we need to split the data into training and testing sets. Last week, we saw how to do this manually (i.e. writing our own code). Sklearn offers a function called train_test_split that you can also use as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "reviews_train, reviews_test, y_train, y_test = train_test_split(reviews, labels, test_size=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Nets\n",
    "\n",
    "The Keras library allows us to quickly define a CNN architecture, train and evaluate a CNN model with minimal effort. But firstly, we will need to introduce a new concept: __Embeddings__\n",
    "\n",
    "1. Word Embedding \n",
    "\n",
    "Word embeddings are approaches for representing words and documents using vectors. Word embeddings offer an improvement over the traditional bag-of-word encoding paradigms where large sparse vectors are used to represent each word or texts. This way of representing words and texts introduce the sparsity problem, because the number of unique words in a document can be vast and a given word or text is represented by a large vector comprised mostly of zero values. \n",
    "\n",
    "Instead, in an embedding, words are represented by __dense vectors__ (smaller vectors with minimal zero values) where a vector represents the projection of the word into a continuous vector space.\n",
    "\n",
    "The position of a word within the vector space is learnt from text and is based on the surrounding words of a word when it is used. The __position of a word__ in the learnt vector space is known as its __embedding__.\n",
    "\n",
    "There are a few popular ways of learning word embeddings from text such as Word2Vec, Doc2Vec, Glove and ELMO. \n",
    "\n",
    "In addition to these popular methods, word embeddings can be learned as part of a deep learning model as we will see below. \n",
    "\n",
    "2. Keras Embedding Layer\n",
    "\n",
    "Keras offers an Embedding layer that can be used for training neural networks using text data. Keras' Embedding layer takes as input integer encoded data, so that each word is represented by a unique integer. We can use tokenisation to pre-process the data. \n",
    "\n",
    "Then, the weights of the Embedding layer are randomly initialized and it is trained to learn an embedding for all of the words in the training dataset.\n",
    "\n",
    "The Embedding layer is defined as the first hidden layer of a network. It must specify 3 arguments:\n",
    "\n",
    "1. __input_dim__: the size of the vocabulary in the text data. For example, if your data is integer encoded to values between 0-199, then the input_dim will be 200.\n",
    "2. __output_dim__: the size of the vector space in which words will be embedded. It is a user-specified value, so test different values for your task.\n",
    "3. __input_length__: This is the length of input sequences, as you would define for any input layer of a Keras model. For example, if all of your input documents are comprised of 1000 words, this would be 1000.\n",
    "\n",
    "For example, below we define an Embedding layer with a vocabulary of 1922 (e.g. integer encoded words from 0 to 1921, inclusive), a vector space of 50 dimensions in which words will be embedded, and input documents that have 50 words each.\n",
    "\n",
    "__Embedding(1922, 50, input_length=30)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the tokenizer: https://keras.io/preprocessing/text/ \n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "\n",
    "#Use tokenisation only on the training data!\n",
    "tokenizer.fit_on_texts(reviews_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(reviews_train)\n",
    "X_test = tokenizer.texts_to_sequences(reviews_test)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
    "\n",
    "print(reviews_train[0])\n",
    "print(X_train[0])\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above method will result in text sequences of variable length of words. To counter this, we can use pad_sequence() which pads the sequence of words with zeros. Additionally you would want to add a maxlen parameter to specify how long the sequences should be. For more oprions of the __pad_sequence__ function look here:  https://keras.io/preprocessing/sequence/ \n",
    "\n",
    "In the following code, you can see how to pad sequences with Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "maxlen = 50\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw during the workbook/lecture, CNNs is just a sequence of different types of layers. Keras allows us to \"build\" this sequence of layers easily. First, we need to define the type of model, in this case Sequential as follows:\n",
    "\n",
    "__model = Sequential()__\n",
    "\n",
    "Then we can add layers as we want, e.g. we can add Convolutional Layers (e.g. Conv1D), Pooling layers (e.g. MaxPooling1D), Fully Connected layers (e.g. Dense) and so on. For a full list of acceptable layers, please see the Keras documentation: https://keras.io/layers/about-keras-layers/ \n",
    "\n",
    "Then you simply add the layers of your choice as follows:\n",
    "\n",
    "__model.add(layers.Dense(1, activation='sigmoid'))__\n",
    "\n",
    "As you see from the example, you can choose the activation function you want as well. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "\n",
    "embedding_dim = 50\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen)) #https://keras.io/layers/embeddings/ \n",
    "model.add(layers.Conv1D(128, 5, activation='relu')) #https://keras.io/layers/convolutional/\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "#spend some time familiarising with the keras documentation. \n",
    "#It is imporssible to remember all the options available but you should be able to remember the basics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now see that we have 129,529 new parameters to train. The 93,950 parameters are derived from vocab_size times the embedding_dim (1879 x 50). These weights of the embedding layer are initialized with random weights and are then adjusted through backpropagation during training. This model takes the words as they come in the order of the sentences as input vectors. You can train it with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = model.fit(X_train, y_train, epochs=10, verbose=False, validation_data=(X_test, y_test), batch_size=10)\n",
    "#details about the model: https://keras.io/models/model/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the accuracy during training and testing as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can observe how fast the model learns by plotting the historical data of accuracy and loss. We can see that our model would reach a good level of accuracy after only three epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "def plot_history(training):\n",
    "    acc = training.history['acc']\n",
    "    val_acc = training.history['val_acc']\n",
    "    loss = training.history['loss']\n",
    "    val_loss = training.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, acc, 'b', label='Training acc')\n",
    "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is a walk through on how to create a train a CNN. As an exercise, familiarise yourself with the Keras exercise and try to add new layers to the CNN architcture / change the values of variables and observe what is happening. You can also try a CNN with the assignment data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
